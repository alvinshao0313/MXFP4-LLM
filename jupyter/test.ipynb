{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7cdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mx.elemwise_ops import quantize_elemwise_op\n",
    "from mx.mx_ops import quantize_mx_op\n",
    "from mx.specs import MxSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419eb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mx_quant_dequant(input, mx_specs, axes=-1):\n",
    "    dtype = input.dtype\n",
    "    # bf_in = quantize_elemwise_op(\n",
    "    #     input.float(), mx_specs=mx_specs, round=mx_specs[\"round_output\"]\n",
    "    # )\n",
    "    qin = quantize_elemwise_op(\n",
    "        input,\n",
    "        mx_specs,\n",
    "        round=mx_specs[\"round_mx_output\"],\n",
    "    )\n",
    "    qin = qin.to(dtype)\n",
    "    return qin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf993781",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_specs = MxSpecs()\n",
    "mx_specs['custom_cuda'] = True\n",
    "mx_specs['w_elem_format'] = 'int4'\n",
    "mx_specs['w_scale_mode'] = 2\n",
    "mx_specs['per_tensor'] = False\n",
    "mx_specs['scale_bits'] = 16\n",
    "mx_specs['block_size'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23b3602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9., 400.,  -1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 400, -1]\n",
    "                    #   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 400, -1],\n",
    "                    #   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 400, -1],\n",
    "                    #   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 400, -1],\n",
    "                     , dtype=torch.float32)\n",
    "quantized_tensor = mx_quant_dequant(tensor.unsqueeze(0), mx_specs, axes=-1)\n",
    "print(quantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42655715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.0000,   0.8571,   2.1429,   3.0000],\n",
      "        [  4.0000,   5.0000,   6.0000,   7.0000],\n",
      "        [  0.0000,   0.0000, 400.0000,  -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "tensor = tensor.reshape(3, 4)\n",
    "q_t = (tensor / (tensor.max(dim=-1, keepdim=True)[0] / 7)).round().clamp(-8, 7) * (tensor.max(dim=-1, keepdim=True)[0] / 7)\n",
    "print(q_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1457da56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_bits: 16\n",
      "w_elem_format: int4\n",
      "a_elem_format: None\n",
      "A_elem_format: None\n",
      "B_elem_format: None\n",
      "w_elem_format_bp: None\n",
      "a_elem_format_bp_ex: None\n",
      "a_elem_format_bp_os: None\n",
      "mx_flush_fp32_subnorms: False\n",
      "shared_exp_method: max\n",
      "block_size: 4\n",
      "bfloat: 0\n",
      "fp: 0\n",
      "bfloat_subnorms: True\n",
      "quantize_backprop: True\n",
      "round: nearest\n",
      "round_m: nearest\n",
      "round_weight: nearest\n",
      "round_output: nearest\n",
      "round_grad_weight: nearest\n",
      "round_grad_input: nearest\n",
      "round_mx_output: nearest\n",
      "round_mx_input_grad_input: nearest\n",
      "round_mx_weight_grad_input: nearest\n",
      "round_mx_grad_output_grad_input: nearest\n",
      "round_mx_input_grad_weight: nearest\n",
      "round_mx_grad_output_grad_weight: nearest\n",
      "softmax_exp2: False\n",
      "vec_use_exp2: False\n",
      "vec_use_recip: False\n",
      "custom_cuda: True\n",
      "a_scale_mode: 0\n",
      "w_scale_mode: 2\n",
      "A_scale_mode: 0\n",
      "B_scale_mode: 0\n",
      "per_tensor: False\n"
     ]
    }
   ],
   "source": [
    "for k, v in mx_specs.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add05f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaoyuantian/anaconda3/envs/mx-qllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shaoyuantian/anaconda3/envs/mx-qllm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:40<00:00, 20.20s/it]\n",
      "/home/shaoyuantian/anaconda3/envs/mx-qllm/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b866fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "smooth = torch.load(\"/home/shaoyuantian/program/MXFP4-LLM/smooth_scales/Meta-Llama-3-8B_per_smooth_scales.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be7c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.0.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.0.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.0.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.0.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.0.mlp.up_proj torch.Size([4096])\n",
      "model.layers.0.mlp.down_proj torch.Size([14336])\n",
      "model.layers.1.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.1.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.1.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.1.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.1.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.1.mlp.up_proj torch.Size([4096])\n",
      "model.layers.1.mlp.down_proj torch.Size([14336])\n",
      "model.layers.2.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.2.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.2.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.2.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.2.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.2.mlp.up_proj torch.Size([4096])\n",
      "model.layers.2.mlp.down_proj torch.Size([14336])\n",
      "model.layers.3.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.3.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.3.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.3.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.3.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.3.mlp.up_proj torch.Size([4096])\n",
      "model.layers.3.mlp.down_proj torch.Size([14336])\n",
      "model.layers.4.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.4.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.4.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.4.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.4.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.4.mlp.up_proj torch.Size([4096])\n",
      "model.layers.4.mlp.down_proj torch.Size([14336])\n",
      "model.layers.5.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.5.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.5.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.5.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.5.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.5.mlp.up_proj torch.Size([4096])\n",
      "model.layers.5.mlp.down_proj torch.Size([14336])\n",
      "model.layers.6.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.6.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.6.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.6.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.6.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.6.mlp.up_proj torch.Size([4096])\n",
      "model.layers.6.mlp.down_proj torch.Size([14336])\n",
      "model.layers.7.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.7.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.7.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.7.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.7.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.7.mlp.up_proj torch.Size([4096])\n",
      "model.layers.7.mlp.down_proj torch.Size([14336])\n",
      "model.layers.8.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.8.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.8.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.8.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.8.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.8.mlp.up_proj torch.Size([4096])\n",
      "model.layers.8.mlp.down_proj torch.Size([14336])\n",
      "model.layers.9.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.9.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.9.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.9.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.9.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.9.mlp.up_proj torch.Size([4096])\n",
      "model.layers.9.mlp.down_proj torch.Size([14336])\n",
      "model.layers.10.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.10.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.10.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.10.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.10.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.10.mlp.up_proj torch.Size([4096])\n",
      "model.layers.10.mlp.down_proj torch.Size([14336])\n",
      "model.layers.11.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.11.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.11.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.11.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.11.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.11.mlp.up_proj torch.Size([4096])\n",
      "model.layers.11.mlp.down_proj torch.Size([14336])\n",
      "model.layers.12.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.12.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.12.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.12.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.12.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.12.mlp.up_proj torch.Size([4096])\n",
      "model.layers.12.mlp.down_proj torch.Size([14336])\n",
      "model.layers.13.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.13.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.13.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.13.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.13.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.13.mlp.up_proj torch.Size([4096])\n",
      "model.layers.13.mlp.down_proj torch.Size([14336])\n",
      "model.layers.14.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.14.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.14.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.14.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.14.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.14.mlp.up_proj torch.Size([4096])\n",
      "model.layers.14.mlp.down_proj torch.Size([14336])\n",
      "model.layers.15.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.15.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.15.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.15.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.15.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.15.mlp.up_proj torch.Size([4096])\n",
      "model.layers.15.mlp.down_proj torch.Size([14336])\n",
      "model.layers.16.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.16.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.16.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.16.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.16.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.16.mlp.up_proj torch.Size([4096])\n",
      "model.layers.16.mlp.down_proj torch.Size([14336])\n",
      "model.layers.17.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.17.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.17.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.17.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.17.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.17.mlp.up_proj torch.Size([4096])\n",
      "model.layers.17.mlp.down_proj torch.Size([14336])\n",
      "model.layers.18.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.18.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.18.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.18.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.18.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.18.mlp.up_proj torch.Size([4096])\n",
      "model.layers.18.mlp.down_proj torch.Size([14336])\n",
      "model.layers.19.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.19.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.19.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.19.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.19.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.19.mlp.up_proj torch.Size([4096])\n",
      "model.layers.19.mlp.down_proj torch.Size([14336])\n",
      "model.layers.20.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.20.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.20.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.20.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.20.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.20.mlp.up_proj torch.Size([4096])\n",
      "model.layers.20.mlp.down_proj torch.Size([14336])\n",
      "model.layers.21.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.21.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.21.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.21.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.21.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.21.mlp.up_proj torch.Size([4096])\n",
      "model.layers.21.mlp.down_proj torch.Size([14336])\n",
      "model.layers.22.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.22.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.22.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.22.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.22.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.22.mlp.up_proj torch.Size([4096])\n",
      "model.layers.22.mlp.down_proj torch.Size([14336])\n",
      "model.layers.23.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.23.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.23.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.23.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.23.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.23.mlp.up_proj torch.Size([4096])\n",
      "model.layers.23.mlp.down_proj torch.Size([14336])\n",
      "model.layers.24.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.24.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.24.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.24.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.24.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.24.mlp.up_proj torch.Size([4096])\n",
      "model.layers.24.mlp.down_proj torch.Size([14336])\n",
      "model.layers.25.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.25.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.25.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.25.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.25.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.25.mlp.up_proj torch.Size([4096])\n",
      "model.layers.25.mlp.down_proj torch.Size([14336])\n",
      "model.layers.26.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.26.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.26.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.26.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.26.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.26.mlp.up_proj torch.Size([4096])\n",
      "model.layers.26.mlp.down_proj torch.Size([14336])\n",
      "model.layers.27.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.27.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.27.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.27.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.27.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.27.mlp.up_proj torch.Size([4096])\n",
      "model.layers.27.mlp.down_proj torch.Size([14336])\n",
      "model.layers.28.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.28.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.28.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.28.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.28.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.28.mlp.up_proj torch.Size([4096])\n",
      "model.layers.28.mlp.down_proj torch.Size([14336])\n",
      "model.layers.29.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.29.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.29.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.29.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.29.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.29.mlp.up_proj torch.Size([4096])\n",
      "model.layers.29.mlp.down_proj torch.Size([14336])\n",
      "model.layers.30.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.30.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.30.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.30.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.30.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.30.mlp.up_proj torch.Size([4096])\n",
      "model.layers.30.mlp.down_proj torch.Size([14336])\n",
      "model.layers.31.self_attn.q_proj torch.Size([4096])\n",
      "model.layers.31.self_attn.k_proj torch.Size([4096])\n",
      "model.layers.31.self_attn.v_proj torch.Size([4096])\n",
      "model.layers.31.self_attn.o_proj torch.Size([4096])\n",
      "model.layers.31.mlp.gate_proj torch.Size([4096])\n",
      "model.layers.31.mlp.up_proj torch.Size([4096])\n",
      "model.layers.31.mlp.down_proj torch.Size([14336])\n",
      "lm_head torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for k in smooth.keys():\n",
    "        print(k, smooth[k].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mx-qllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
